{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      },
      "source": [
        "# Finetune LLM with LLaMA Factory\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr7rB3szzhtx"
      },
      "source": [
        "## Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giM74oK1rRIH",
        "outputId": "8096a052-441f-4d2c-d122-4b310f4187e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 475, done.\u001b[K\n",
            "remote: Counting objects: 100% (475/475), done.\u001b[K\n",
            "remote: Compressing objects: 100% (386/386), done.\u001b[K\n",
            "remote: Total 475 (delta 122), reused 294 (delta 73), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (475/475), 5.17 MiB | 17.65 MiB/s, done.\n",
            "Resolving deltas: 100% (122/122), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/    Makefile        README.md         \u001b[01;34mscripts\u001b[0m/  \u001b[01;34mtests\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mexamples\u001b[0m/  MANIFEST.in     README_zh.md      setup.py  \u001b[01;34mtests_v1\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         LICENSE    pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers!=4.52.0,!=4.57.0,<=4.57.3,>=4.49.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (4.57.3)\n",
            "Requirement already satisfied: datasets<=4.0.0,>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (4.0.0)\n",
            "Requirement already satisfied: accelerate<=1.11.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (1.11.0)\n",
            "Requirement already satisfied: peft<=0.17.1,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.17.1)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.9.6)\n",
            "Requirement already satisfied: gradio<=5.45.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (5.45.0)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (3.10.0)\n",
            "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.8.14)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (1.26.4)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (1.16.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.2.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.12.0)\n",
            "Requirement already satisfied: modelscope>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (1.33.0)\n",
            "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.1.9)\n",
            "Requirement already satisfied: safetensors<=0.5.3 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.5.3)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.7.1)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (6.0.3)\n",
            "Requirement already satisfied: pydantic<=2.10.6 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (2.10.6)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.38.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.118.3)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (3.0.3)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (16.0.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.11.0)\n",
            "Requirement already satisfied: propcache!=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.4.1)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.24.0+cu126)\n",
            "Requirement already satisfied: bitsandbytes>=0.39.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.49.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (2025.3.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.2.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.13.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.13.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.11.5)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.14.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.15.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from modelscope>=1.14.0->llamafactory==0.9.4.dev0) (75.2.0)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.12/dist-packages (from modelscope>=1.14.0->llamafactory==0.9.4.dev0) (2.5.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.4.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.4.dev0) (2.27.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (3.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=4.57.3,>=4.49.0->llamafactory==0.9.4.dev0) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=4.57.3,>=4.49.0->llamafactory==0.9.4.dev0) (0.22.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (1.8.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn->llamafactory==0.9.4.dev0) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn->llamafactory==0.9.4.dev0) (0.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->llamafactory==0.9.4.dev0) (3.2.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.0.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.1.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->llamafactory==0.9.4.dev0) (4.9.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.11)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.13.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.0.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (1.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.4.dev0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.4.dev0) (4.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (2.19.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.4.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->llamafactory==0.9.4.dev0) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (6.7.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.22.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (2.23)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (0.1.2)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.4.dev0-0.editable-py3-none-any.whl size=28983 sha256=68669a8ba8d20b4b2704a054cbf900aeb3bdf0edfa1d507e5b0efbd477080799\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qvkd63ww/wheels/68/8b/5e/52f9888e6a91a2651260d603137c052b925af896da6e32a3f7\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: llamafactory\n",
            "  Attempting uninstall: llamafactory\n",
            "    Found existing installation: llamafactory 0.9.4.dev0\n",
            "    Uninstalling llamafactory-0.9.4.dev0:\n",
            "      Successfully uninstalled llamafactory-0.9.4.dev0\n",
            "Successfully installed llamafactory-0.9.4.dev0\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KllqrHDJV2ma"
      },
      "source": [
        "## Step 2: Register Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLU6KEgFUHIV",
        "outputId": "84fd48d2-0fb7-4605-b859-b9dd66e1d205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Found config file at: /content/LLaMA-Factory/data/dataset_info.json\n",
            "âœ… Found your dataset at: /content/LLaMA-Factory/data/asset_management_broad_v2.json\n",
            "\n",
            "ðŸŽ‰ Success! 'asset_management_train' is registered.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# 1. Use the ABSOLUTE path so it never gets lost\n",
        "# (This assumes you are on Google Colab defaults)\n",
        "config_path = \"/content/LLaMA-Factory/data/dataset_info.json\"\n",
        "dataset_path = \"/content/LLaMA-Factory/data/asset_management_broad_v2.json\"\n",
        "\n",
        "# Debugging: Verify the files actually exist before trying to open\n",
        "if not os.path.exists(config_path):\n",
        "    print(f\"âŒ Error: Config file not found at {config_path}\")\n",
        "    print(\"   Please verify you cloned the repo successfully.\")\n",
        "else:\n",
        "    print(f\"âœ… Found config file at: {config_path}\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(f\"âŒ Error: Your JSON file is missing at {dataset_path}\")\n",
        "    print(\"   Make sure you dragged 'asset_management_broad_v2.json' into the 'data' folder!\")\n",
        "else:\n",
        "    print(f\"âœ… Found your dataset at: {dataset_path}\")\n",
        "\n",
        "# 2. Only proceed if files exist\n",
        "if os.path.exists(config_path) and os.path.exists(dataset_path):\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data_info = json.load(f)\n",
        "\n",
        "    # 3. Add your specific dataset to the list\n",
        "    data_info[\"asset_management_broad_v2\"] = {\n",
        "        \"file_name\": \"asset_management_broad_v2.json\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # 4. Save the file back\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data_info, f, indent=2)\n",
        "\n",
        "    print(\"\\nðŸŽ‰ Success! 'asset_management_train' is registered.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE8R2wWFsJGf"
      },
      "source": [
        "## Step 3: Connect Hugging Face Token to Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZngJTA8sRlV"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your token inside the quotes\n",
        "login(token=\"insert token\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QiXcvdzzW3Y"
      },
      "source": [
        "## Step 4: Fine-tune model via LLaMA Board. Run cell and select \"Running on public URL\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLsdS6V5yUMy",
        "outputId": "4ad7c8f2-e82c-4b08-8f02-3079202e7d3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-12-09 09:56:01.556765: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765274161.576515    1673 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765274161.582448    1673 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765274161.597767    1673 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765274161.597797    1673 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765274161.597801    1673 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765274161.597804    1673 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-09 09:56:01.602557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n",
            "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
            "* Running on local URL:  http://0.0.0.0:7860\n",
            "* Running on public URL: https://1c42dad50e8383059f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "[INFO|2025-12-09 10:00:13] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|2025-12-09 10:00:23] llamafactory.data.template:143 >> Add <eos>,<end_of_turn> to stop words.\n",
            "[INFO|2025-12-09 10:00:23] llamafactory.data.loader:143 >> Loading dataset asset_management_train.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[2, 106, 1645, 108, 2045, 708, 671, 14762, 53060, 20409, 235265, 100715, 573, 3356, 4680, 235269, 3277, 235269, 578, 5966, 578, 8363, 1013, 665, 603, 476, 8890, 577, 1536, 235290, 5584, 12881, 4731, 575, 44258, 7219, 235265, 10358, 675, 235248, 235274, 604, 7778, 235269, 235248, 235276, 604, 793, 235265, 108, 11397, 14561, 235292, 62036, 16509, 31289, 108, 13852, 235292, 11158, 6226, 576, 6591, 108, 5360, 235292, 56615, 10575, 235269, 7062, 108, 3602, 235292, 8045, 573, 3356, 235316, 108, 11397, 13705, 235316, 108, 235316, 108, 11397, 8290, 235316, 108, 235316, 108, 1841, 603, 573, 7251, 103026, 108, 235316, 108, 2169, 476, 62036, 16509, 31289, 675, 112890, 62036, 7219, 235269, 692, 877, 614, 1702, 576, 573, 62036, 16509, 31289, 591, 200434, 235275, 6252, 235265, 1417, 603, 476, 235248, 235284, 235310, 235290, 10533, 3505, 2733, 6869, 577, 3658, 573, 5567, 578, 7841, 4647, 577, 21252, 696, 112890, 235265, 25504, 17230, 576, 573, 2733, 235269, 692, 877, 791, 573, 7251, 577, 8407, 476, 17281, 50890, 2970, 577, 6824, 476, 14097, 3281, 575, 2676, 576, 573, 8344, 235303, 235256, 9082, 235290, 6576, 13712, 4815, 9803, 6583, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 1734, 52837, 611, 573, 10247, 577, 614, 1167, 1963, 235269, 29102, 7166, 577, 2745, 8547, 235269, 578, 3434, 3584, 577, 6824, 31217, 8111, 577, 1707, 1167, 9325, 52837, 578, 11707, 23522, 235265, 1448, 2212, 1105, 1853, 1156, 235269, 20772, 1167, 5736, 235269, 3547, 476, 5830, 577, 1167, 11707, 235269, 578, 32379, 3361, 674, 603, 24185, 6583, 108, 235316, 108, 235316, 108, 235316, 108, 651, 1426, 235290, 56841, 5043, 18463, 3001, 604, 573, 3131, 3668, 603, 697, 235308, 235276, 235269, 235276, 235276, 235276, 728, 697, 235321, 235308, 235269, 235276, 235276, 235276, 13478, 611, 7549, 3359, 901, 780, 6929, 577, 573, 16152, 235349, 235256, 3281, 235269, 7841, 235269, 16014, 4938, 235289, 3952, 4202, 235289, 578, 2567, 4026, 235265, 1417, 18463, 3001, 1721, 780, 3707, 1156, 6635, 576, 3051, 19355, 235269, 3359, 476, 117337, 17777, 578, 7839, 1582, 685, 476, 235248, 235310, 235276, 235274, 235278, 235273, 235275, 2733, 675, 3277, 235290, 68121, 16426, 235289, 2962, 235269, 21470, 235269, 9678, 235269, 1913, 578, 30403, 9490, 235289, 578, 6838, 1069, 235290, 2672, 1780, 6583, 108, 235316, 108, 183863, 235349, 235256, 19355, 19754, 578, 12555, 17917, 573, 9156, 576, 476, 7475, 19767, 5228, 40891, 578, 12258, 476, 9523, 4731, 575, 67340, 235269, 30509, 578, 53017, 16850, 674, 3708, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235315, 235310, 235318, 235308, 51860, 169645, 117671, 85035, 6320, 235292, 7434, 5507, 9042, 191221, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 143662, 21648, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 18692, 3858, 576, 5783, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235310, 235276, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 7680, 1069, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 6948, 29259, 49907, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 29144, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 6122, 162602, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235284, 235276, 235284, 235308, 235290, 235276, 235324, 235290, 235284, 235321, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 235316, 108, 2761, 112890, 235269, 783, 4564, 671, 28540, 30464, 674, 919, 16932, 39438, 603, 8131, 577, 1167, 7822, 5115, 685, 974, 576, 573, 10155, 578, 1546, 8385, 14183, 575, 573, 2134, 235265, 125857, 476, 30464, 1570, 1167, 8663, 2375, 10005, 577, 3468, 108, 11227, 736, 3356, 3806, 970, 12830, 235336, 107, 108, 106, 2516, 108, 235276, 107, 108, 1]\n",
            "inputs:\n",
            "<bos><start_of_turn>user\n",
            "You are an executive recruiting assistant. Analyze the job title, company, and description and determine if it is a medium to high-level leadership role in Asset Management. Answer with 1 for yes, 0 for no.\n",
            "Job Title: Wealth Planning Associate\n",
            "Company: Royal Bank of Canada\n",
            "Location: Newport Beach, CA\n",
            "Description: About the job\n",
            "Job Summary\n",
            "\n",
            "Job Description\n",
            "\n",
            "What is the opportunity?\n",
            "\n",
            "As a Wealth Planning Associate with RBC Wealth Management, you will be part of the Wealth Planning Associate (WPA) Program. This is a 24-month development program designed to provide the knowledge and skills necessary to succeed at RBC. Upon completion of the program, you will have the opportunity to join a Financial Advisor team to deliver a consistent experience in support of the organization's goals-based wealth management strategy.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "We thrive on the challenge to be our best, progressive thinking to keep growing, and working together to deliver trusted advice to help our clients thrive and communities prosper. We care about each other, reaching our potential, making a difference to our communities, and achieving success that is mutual.\n",
            "\n",
            "\n",
            "\n",
            "The good-faith expected salary range for the above position is $50,000 - $85,000 depending on factors including but not limited to the candidateâ€™s experience, skills, registration status; market conditions; and business needs. This salary range does not include other elements of total compensation, including a discretionary bonus and benefits such as a 401(k) program with company-matching contributions; health, dental, vision, life and disability insurance; and paid time-off plan.\n",
            "\n",
            "RBCâ€™s compensation philosophy and principles recognize the importance of a highly qualified global workforce and plays a critical role in attracting, engaging and retaining talent that:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "9465 WILSHIRE BOULEVARD:BEVERLY HILLS\n",
            "\n",
            "\n",
            "\n",
            "Beverly Hills\n",
            "\n",
            "\n",
            "\n",
            "United States of America\n",
            "\n",
            "\n",
            "\n",
            "40\n",
            "\n",
            "\n",
            "\n",
            "Full time\n",
            "\n",
            "\n",
            "\n",
            "WEALTH MANAGEMENT\n",
            "\n",
            "\n",
            "\n",
            "Regular\n",
            "\n",
            "\n",
            "\n",
            "Salaried\n",
            "\n",
            "\n",
            "\n",
            "2025-07-28\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "At RBC, we believe an inclusive workplace that has diverse perspectives is core to our continued growth as one of the largest and most successful banks in the world. Maintaining a workplace where our employees feel supported to perf\n",
            "Does this job fit my criteria?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "0<end_of_turn>\n",
            "<eos>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 235276, 107, 108, 1]\n",
            "labels:\n",
            "0<end_of_turn>\n",
            "<eos>\n",
            "[WARNING|2025-12-09 10:00:56] llamafactory.model.model_utils.attention:148 >> FlashAttention-2 is not installed, use eager attention.\n",
            "[INFO|2025-12-09 10:00:56] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "2025-12-09 10:00:05.769509: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765274405.789337    2726 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765274405.795455    2726 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765274405.810857    2726 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765274405.810881    2726 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765274405.810886    2726 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765274405.810888    2726 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 10:00:18,633 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 10:00:18,633 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 10:00:18,633 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 10:00:18,633 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 10:00:18,633 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 10:00:18,634 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:765] 2025-12-09 10:00:21,503 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-12-09 10:00:21,506 >> Model config Gemma2Config {\n",
            "  \"architectures\": [\n",
            "    \"Gemma2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attn_logit_softcapping\": 50.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"cache_implementation\": \"hybrid\",\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"final_logit_softcapping\": 30.0,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"layer_types\": [\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma2\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 42,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"query_pre_attn_scalar\": 256,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"sliding_window_size\": 4096,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 10:00:21,990 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 10:00:21,990 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 10:00:21,991 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 10:00:21,991 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 10:00:21,991 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2095] 2025-12-09 10:00:21,991 >> loading file chat_template.jinja from cache at None\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "WARNING:datasets.builder:Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "\n",
            "Generating train split: 0 examples [00:00, ? examples/s]\n",
            "Generating train split: 340 examples [00:00, 7100.25 examples/s]\n",
            "\n",
            "Converting format of dataset (num_proc=16):   0%|          | 0/340 [00:00<?, ? examples/s]\n",
            "Converting format of dataset (num_proc=16):   2%|â–         | 6/340 [00:00<00:32, 10.29 examples/s]\n",
            "Converting format of dataset (num_proc=16):  21%|â–ˆâ–ˆ        | 71/340 [00:00<00:02, 127.17 examples/s]\n",
            "Converting format of dataset (num_proc=16):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 151/340 [00:00<00:00, 252.90 examples/s]\n",
            "Converting format of dataset (num_proc=16):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 268/340 [00:00<00:00, 437.25 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:01<00:00, 249.87 examples/s]\n",
            "\n",
            "Running tokenizer on dataset (num_proc=16):   0%|          | 0/340 [00:00<?, ? examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 22/340 [00:04<01:10,  4.51 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  13%|â–ˆâ–Ž        | 44/340 [00:06<00:42,  7.03 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  19%|â–ˆâ–‰        | 66/340 [00:08<00:31,  8.81 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  26%|â–ˆâ–ˆâ–Œ       | 88/340 [00:11<00:30,  8.34 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  32%|â–ˆâ–ˆâ–ˆâ–      | 109/340 [00:13<00:24,  9.44 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 130/340 [00:14<00:20, 10.43 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 151/340 [00:16<00:17, 11.06 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 172/340 [00:18<00:14, 11.43 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 193/340 [00:19<00:12, 11.55 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 214/340 [00:21<00:10, 11.85 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 235/340 [00:23<00:09, 10.62 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 256/340 [00:25<00:07, 10.65 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 277/340 [00:26<00:05, 12.22 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 298/340 [00:28<00:03, 13.87 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 319/340 [00:28<00:01, 16.69 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:29<00:00, 19.80 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:30<00:00, 11.30 examples/s]\n",
            "[INFO|configuration_utils.py:765] 2025-12-09 10:00:56,237 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-12-09 10:00:56,238 >> Model config Gemma2Config {\n",
            "  \"architectures\": [\n",
            "    \"Gemma2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attn_logit_softcapping\": 50.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"cache_implementation\": \"hybrid\",\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"final_logit_softcapping\": 30.0,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"layer_types\": [\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\",\n",
            "    \"sliding_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma2\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 42,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"query_pre_attn_scalar\": 256,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"sliding_window_size\": 4096,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2025-12-09 10:00:57,834 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[INFO|modeling_utils.py:1172] 2025-12-09 10:00:58,565 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/model.safetensors.index.json\n",
            "\n",
            "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]\n",
            "Fetching 4 files:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [05:53<17:41, 353.93s/it]\n",
            "Fetching 4 files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [05:57<04:55, 147.86s/it]\n",
            "Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [05:57<00:00, 89.39s/it] \n",
            "[INFO|modeling_utils.py:2341] 2025-12-09 10:06:56,407 >> Instantiating Gemma2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[WARNING|logging.py:328] 2025-12-09 10:06:56,410 >> The following generation flags are not valid and may be ignored: ['cache_implementation'].\n",
            "[INFO|logging.py:343] 2025-12-09 10:06:56,410 >> - `cache_implementation`: You have set `use_cache` to `False`, but cache_implementation is set to hybrid. cache_implementation will have no effect.\n",
            "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
            "[INFO|configuration_utils.py:986] 2025-12-09 10:06:56,410 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"cache_implementation\": \"hybrid\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:24<01:14, 24.72s/it]\n",
            "Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:48<00:48, 24.31s/it]\n",
            "Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:12<00:24, 24.12s/it]\n",
            "Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:16<00:25, 25.37s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 24, in main\n",
            "    launcher.launch()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/launcher.py\", line 157, in launch\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 132, in run_exp\n",
            "    _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 93, in _training_function\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 52, in run_sft\n",
            "    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/model/loader.py\", line 179, in load_model\n",
            "    model = load_class.from_pretrained(**init_kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 5048, in from_pretrained\n",
            "    ) = cls._load_pretrained_model(\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 5468, in _load_pretrained_model\n",
            "    _error_msgs, disk_offload_index = load_shard_file(args)\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 843, in load_shard_file\n",
            "    disk_offload_index = _load_state_dict_into_meta_model(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 748, in _load_state_dict_into_meta_model\n",
            "    param = param[...]\n",
            "            ~~~~~^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 82.12 MiB is free. Process 15555 has 102.00 MiB memory in use. Process 28520 has 14.56 GiB memory in use. Of the allocated memory 14.45 GiB is allocated by PyTorch, and 9.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udce3kCwWlYL"
      },
      "source": [
        "# Extra Features from Original Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9RXn_YQnn9f"
      },
      "source": [
        "### Check GPU environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeYs5Lz-QJYk"
      },
      "source": [
        "## Update Identity Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap_fvMBsQHJc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"Llama-3\"\n",
        "AUTHOR = \"LLaMA Factory\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      },
      "source": [
        "## Fine-tune model via Command Line\n",
        "\n",
        "It takes ~30min for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS0Qk5OR0i4Q"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                                               # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  dataset=\"identity,alpaca_en_demo\",                         # use alpaca and identity datasets\n",
        "  template=\"llama3\",                                         # use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                                    # use LoRA adapters to save memory\n",
        "  lora_target=\"all\",                                         # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"llama3_lora\",                                  # the path to save LoRA adapters\n",
        "  per_device_train_batch_size=2,                             # the micro batch size\n",
        "  gradient_accumulation_steps=4,                             # the gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                                # use cosine learning rate scheduler\n",
        "  logging_steps=5,                                           # log every 5 steps\n",
        "  warmup_ratio=0.1,                                          # use warmup scheduler\n",
        "  save_steps=1000,                                           # save checkpoint every 1000 steps\n",
        "  learning_rate=5e-5,                                        # the learning rate\n",
        "  num_train_epochs=3.0,                                      # the epochs of training\n",
        "  max_samples=500,                                           # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                                         # clip gradient norm to 1.0\n",
        "  loraplus_lr_ratio=16.0,                                    # use LoRA+ algorithm with lambda=16.0\n",
        "  fp16=True,                                                 # use float16 mixed precision training\n",
        "  report_to=\"none\",                                          # disable wandb logging\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVNaC-xS5N40"
      },
      "source": [
        "## Infer the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh8H9A_25SF9"
      },
      "outputs": [],
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",                        # load the saved LoRA adapters\n",
        "  template=\"llama3\",                                         # same to the one in training\n",
        "  finetuning_type=\"lora\",                                    # same to the one in training\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"History has been removed.\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTESHaFvbNTr"
      },
      "source": [
        "## Merge the LoRA adapter and optionally upload model\n",
        "\n",
        "NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcNcHcA4bf4Z"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMojogHbaOZF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",                       # load the saved LoRA adapters\n",
        "  template=\"llama3\",                                        # same to the one in training\n",
        "  finetuning_type=\"lora\",                                   # same to the one in training\n",
        "  export_dir=\"llama3_lora_merged\",                          # the path to save the merged model\n",
        "  export_size=2,                                            # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cpu\",                                      # the device used in export, can be chosen from `cpu` and `auto`\n",
        "  # export_hub_model_id=\"your_id/your_model\",               # the Hugging Face hub ID to upload model\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}